    // word count
    get_time( begin );

    size_t num_files = dir_list.size();
    std::vector<word_list_type> catalog;
    catalog.resize( num_files );

    asap::word_container_reducer<word_map_type2> allwords;
    cilk_for( size_t i=0; i < num_files; ++i ) {
	std::string filename = *std::next(dir_list.cbegin(),i);
	// std::cerr << "Read file " << filename;
	{
	    // Build up catalog for each file using a map
	    word_map_type wmap;
	    asap::word_catalog( std::string(*std::next(dir_list.cbegin(),i)),
				wmap ); // catalog[i] );
	    // Convert file's catalog to a (sorted) list of pairs
	    catalog[i].reserve( wmap.size() );    // avoid re-allocations
	    catalog[i].insert( std::move(wmap) ); // move out wmap contents

	    // The list of pairs is sorted if word_map_type is based on std::map
	    // but it is not sorted if based on std::unordered_map
	    if( do_sort )
		std::sort( catalog[i].begin(), catalog[i].end(),
			   asap::pair_cmp<word_map_type::value_type,
			   word_map_type::value_type>() );
	} // delete wmap

	// std::cerr << ": " << catalog[i].size() << " words\n";
	// Reading from std::vector rather than std::map should be faster...
	// Validated: about 10% on word count, 20% on TF/IDF, 16 threads
	allwords.count_presence( catalog[i] );
    }
    get_time (end);
    print_time("word count", begin, end);

    get_time( begin );

    // TODO: consider linearising the word_map to a word_list with exchanged
    //       word_bank in order to avoid storing the ID? Problem: lookup
    //       during TF/IDF computation
    // TODO: infer word_map_type2 from word_map_type* in template definition?
    // TODO: construct aggregate word_map_type2 during wc loop above
    std::shared_ptr<word_map_type2> allwords_ptr
	= std::make_shared<word_map_type2>();
    allwords_ptr->swap( allwords.get_value() );

    data_set_type data_set;
    if( by_words ) {
	data_set = asap::tfidf_by_words<vector_type>(
	    catalog.cbegin(), catalog.cend(), allwords_ptr,
	    do_sort ); // whether catalogs are sorted
    } else {
	data_set = asap::tfidf<vector_type>(
	    catalog.cbegin(), catalog.cend(), allwords_ptr,
	    do_sort ); // whether catalogs are sorted
    }
    get_time( end );
    print_time("TF/IDF", begin, end);
    std::cerr << "TF/IDF number of words: " << data_set.get_dimensions()
	      << "\nTF/IDF number of files: " << data_set.get_num_points()
	      << std::endl;

    // Normalize data for improved clustering results
    get_time( begin );
    std::vector<std::pair<float, float>> extrema
	= asap::normalize( data_set );
    get_time( end );
    print_time("normalize", begin, end);

    // K-means clustering
    get_time( begin );
    auto kmeans_op = asap::kmeans( data_set, num_clusters, max_iters );
    get_time( end );
    print_time("K-Means", begin, end);
    std::cerr << "K-Means iterations: " << kmeans_op.num_iterations()
	      << "\nK-Means within-cluster SSE: " << kmeans_op.within_sse()
	      << std::endl;

    // Unscale data
    get_time( begin );
    asap::denormalize( extrema, data_set );
    get_time( end );        
    print_time("denormalize", begin, end);
